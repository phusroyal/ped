{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phusr/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotMyModel(\n",
       "  (network): iGPT(\n",
       "    (wte_i): Embedding(50304, 512)\n",
       "    (wpe_i): Embedding(256, 512)\n",
       "    (blocks_i): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (idea_head): Linear(in_features=512, out_features=768, bias=False)\n",
       "    (wte_g): Embedding(50304, 768)\n",
       "    (wpe_g): Embedding(256, 768)\n",
       "    (blocks_g): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_g): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.iGPT import iGPT,iGPTConfig, NotMyModel\n",
    "import torch\n",
    "\n",
    "config = iGPTConfig(\n",
    "    block_size=256,\n",
    "    vocab_size=50304,\n",
    "    n_layer_main=12,\n",
    "    n_layer_idea=12,\n",
    "    n_head=8,\n",
    "    n_embd_main = 768,          \n",
    "    n_embd_idea = 512,\n",
    "    idea_dim=768,\n",
    ")\n",
    "\n",
    "# model = NotMyModel(config)\n",
    "model = NotMyModel.load_from_checkpoint(\"experiments/lightning_logs/version_0/checkpoints/epoch=5-step=14149.ckpt\")\n",
    "# model.cuda()\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# Load the base encoding\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eos = 50257\n",
    "# Define new special tokens\n",
    "new_special_tokens = {\n",
    "    \"<|endofsent|>\": eos,  # Make sure this ID does not conflict with existing tokens\n",
    "}\n",
    "# Create a new encoding with the added special tokens\n",
    "extended_enc = tiktoken.Encoding(\n",
    "    name=\"gpt2_extended\",\n",
    "    pat_str=enc._pat_str,  # Use the same pattern as the original encoding\n",
    "    mergeable_ranks=enc._mergeable_ranks,  # Keep the same mergeable ranks\n",
    "    special_tokens={**enc._special_tokens, **new_special_tokens},  # Extend special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 50\n",
    "# mysent = \"He is credited as a writer on Halo 3 .\"\n",
    "mysent = \"Among the places he has lived since retirement are California and Grants Pass , Oregon .\"\n",
    "sent = extended_enc.encode(mysent)\n",
    "sent = torch.tensor(sent, dtype=torch.long) # (8,)\n",
    "sent = sent.unsqueeze(0)\n",
    "sent = sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_sent = extended_enc.encode(\"<|endofsent|>\", allowed_special={'<|endofsent|>'})\n",
    "init_sent = torch.tensor(init_sent, dtype=torch.long) # (8,)\n",
    "init_sent = init_sent.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "init_sent = init_sent.cuda()\n",
    "init_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "while init_sent.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    # with torch.no_grad():\n",
    "        logits = model((init_sent, sent)) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        init_sent = torch.cat((init_sent, xcol), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among the places he has lived since retirement are California and Grants Pass , Oregon .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <|endofsent|>For the season in two years as a backup actress to appear between April 31 , 2015 through mid @-@ season episodes , with multiple DVDs and low @-@ ratings shows .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issues it became popular with a new brand including limited exclusive online provider ( EP sales ) , its retail @-@ based content and quality sales amounted to 2 @.<|endofsent|>.<|endofsent|><|endofsent|><|endofsent|>.<|endofsent|><|endofsent|>.<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issue he started work and would be shown at various social organizations including : Atlanta University ( where Denny is mentioned as <unk> ) and Austin Cooder University .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the season he was also a substitute ; former NHL champion Du Bois appeared in mid @-@ September , leading him with 10 assists and 20 assists and 15 doubles .<|endofsent|>gers .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the week it became very popular for artists across Brazil , from several poets , and musicians including the artists , ethnographers J. Mait , and many Canadian musicians supported worldwide .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n"
     ]
    }
   ],
   "source": [
    "# small models (4 layers decoder, 2 layer encoder) - 4 epochs\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = init_sent[i, :max_length].tolist()\n",
    "    decoded = extended_enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# lightning module\n",
    "from model.iGPT import NotMyModel  # import your LightningModule class\n",
    "# same code for HellaSwag download + rendering as in your GPT-2 script\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(\"data\"), \"hellaswag\")\n",
    "hellaswags = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "}\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def download_file(url: str, fname: str, chunk_size=1024):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(fname, \"wb\") as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit=\"iB\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def download(split):\n",
    "    \"\"\"Downloads HellaSwag into DATA_CACHE_DIR if not already present.\"\"\"\n",
    "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "    data_url = hellaswags[split]\n",
    "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "    if not os.path.exists(data_filename):\n",
    "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "        download_file(data_url, data_filename)\n",
    "\n",
    "def iterate_examples(split):\n",
    "    download(split)\n",
    "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "\n",
    "def render_example(example):\n",
    "    \"\"\"Same as your GPT-2 code, returns (data, tokens, mask, label).\"\"\"\n",
    "    ctx = example[\"ctx\"]\n",
    "    label = example[\"label\"]\n",
    "    endings = example[\"endings\"]\n",
    "\n",
    "    data = {\n",
    "        \"label\": label,\n",
    "        \"ctx_tokens\": None,\n",
    "        \"ending_tokens\": [],\n",
    "    }\n",
    "    # Tokenize context\n",
    "    ctx_tokens = enc.encode(ctx)\n",
    "    data[\"ctx_tokens\"] = ctx_tokens\n",
    "\n",
    "    # Tokenize each of the 4 endings\n",
    "    tok_rows = []\n",
    "    mask_rows = []\n",
    "    for end in endings:\n",
    "        end_tokens = enc.encode(\" \" + end)  # note: leading space\n",
    "        tok_rows.append(ctx_tokens + end_tokens)\n",
    "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "        data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "    # Collate into a 4 x max_len shape\n",
    "    max_len = max(len(row) for row in tok_rows)\n",
    "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "    return data, tokens, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_iGPT(model_module, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Loads your trained iGPT model from a Lightning checkpoint, then\n",
    "    evaluates on the HellaSwag val set. We measure two scores:\n",
    "\n",
    "    1) 'acc' using sum of cross-entropy loss over each candidate (pred = argmin sum).\n",
    "    2) 'acc_norm' using average cross-entropy over each candidate region (pred = argmin mean).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # For convenience, we'll directly call model_module.network() \n",
    "    # inside our loop below. If your model requires anything special \n",
    "    # for generation or inference, adapt as needed.\n",
    "\n",
    "    # 2) Setup counters\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # 3) Iterate over HellaSwag val examples\n",
    "    for example in iterate_examples(\"val\"):\n",
    "        data, tokens, mask, label = render_example(example)\n",
    "        tokens = tokens.to(device)  # shape: (4, max_len)\n",
    "        mask = mask.to(device)      # shape: (4, max_len)\n",
    "\n",
    "        # iGPT expects two inputs: x, ix\n",
    "        # We'll feed 'tokens' as x, and supply a dummy 'ix' \n",
    "        # because we don't have any \"idea tokens\" at test time.\n",
    "        B, T = tokens.size()  # B = 4\n",
    "        ix_dummy = torch.zeros((B, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass to get logits: shape (B, T, vocab_size)\n",
    "        logits = model_module.network(tokens, ix_dummy)\n",
    "\n",
    "        # 4) Autoregressive loss\n",
    "        # same procedure as the GPT-2 script\n",
    "        shift_logits = logits[..., :-1, :].contiguous()   # (4, T-1, vocab_size)\n",
    "        shift_tokens = tokens[..., 1:].contiguous()       # (4, T-1)\n",
    "        flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "        flat_shift_tokens = shift_tokens.view(-1)\n",
    "        shift_losses = F.cross_entropy(\n",
    "            flat_shift_logits, \n",
    "            flat_shift_tokens,\n",
    "            reduction='none'\n",
    "        )\n",
    "        shift_losses = shift_losses.view(B, -1)  # shape (4, T-1)\n",
    "\n",
    "        # apply the mask\n",
    "        shift_mask = mask[..., 1:].contiguous()  # also shape (4, T-1)\n",
    "        masked_shift_losses = shift_losses * shift_mask\n",
    "\n",
    "        # sum/average over the candidate region\n",
    "        sum_loss = masked_shift_losses.sum(dim=1)          # shape (4,)\n",
    "        avg_loss = sum_loss / shift_mask.sum(dim=1)        # shape (4,)\n",
    "        \n",
    "        # the best candidate is the one with the lowest sum_loss\n",
    "        pred = sum_loss.argmin().item()\n",
    "        pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "        # 5) Accumulate stats\n",
    "        num_total += 1\n",
    "        num_correct += int(pred == label)\n",
    "        num_correct_norm += int(pred_norm == label)\n",
    "\n",
    "        # (Optional) debug prints\n",
    "        # if num_total <= 10:\n",
    "        #     print(\"---\")\n",
    "        #     print(f\"Context:\\n {example['ctx']}\")\n",
    "        #     print(\"Endings:\")\n",
    "        #     for i, end in enumerate(example[\"endings\"]):\n",
    "        #         print(f\"{i} (loss: {avg_loss[i].item():.4f}) => {end}\")\n",
    "        #     print(f\"Predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "        # (Optional) show running accuracy\n",
    "        if num_total % 100 == 0:\n",
    "            print(f\"Processed {num_total} examples. \" \n",
    "                  f\"acc: {num_correct/num_total:.4f}, \"\n",
    "                  f\"acc_norm: {num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "    # 6) Final results\n",
    "    print(f\"Done! Evaluated {num_total} examples.\")\n",
    "    acc = num_correct/num_total\n",
    "    acc_norm = num_correct_norm/num_total\n",
    "    print(f\"Final Acc: {acc:.4f}  Acc_norm: {acc_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to hellaswag/hellaswag_val.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 64.4MiB/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples. acc: 0.2500, acc_norm: 0.1800\n",
      "Processed 200 examples. acc: 0.2750, acc_norm: 0.2450\n",
      "Processed 300 examples. acc: 0.2867, acc_norm: 0.2667\n",
      "Processed 400 examples. acc: 0.2650, acc_norm: 0.2725\n",
      "Processed 500 examples. acc: 0.2640, acc_norm: 0.2680\n",
      "Processed 600 examples. acc: 0.2950, acc_norm: 0.2867\n",
      "Processed 700 examples. acc: 0.2871, acc_norm: 0.2800\n",
      "Processed 800 examples. acc: 0.2750, acc_norm: 0.2900\n",
      "Processed 900 examples. acc: 0.2756, acc_norm: 0.2967\n",
      "Processed 1000 examples. acc: 0.2720, acc_norm: 0.2910\n",
      "Processed 1100 examples. acc: 0.2682, acc_norm: 0.2882\n",
      "Processed 1200 examples. acc: 0.2717, acc_norm: 0.2850\n",
      "Processed 1300 examples. acc: 0.2746, acc_norm: 0.2815\n",
      "Processed 1400 examples. acc: 0.2729, acc_norm: 0.2800\n",
      "Processed 1500 examples. acc: 0.2653, acc_norm: 0.2733\n",
      "Processed 1600 examples. acc: 0.2637, acc_norm: 0.2669\n",
      "Processed 1700 examples. acc: 0.2718, acc_norm: 0.2641\n",
      "Processed 1800 examples. acc: 0.2756, acc_norm: 0.2650\n",
      "Processed 1900 examples. acc: 0.2711, acc_norm: 0.2642\n",
      "Processed 2000 examples. acc: 0.2730, acc_norm: 0.2640\n",
      "Processed 2100 examples. acc: 0.2714, acc_norm: 0.2690\n",
      "Processed 2200 examples. acc: 0.2736, acc_norm: 0.2677\n",
      "Processed 2300 examples. acc: 0.2748, acc_norm: 0.2696\n",
      "Processed 2400 examples. acc: 0.2812, acc_norm: 0.2700\n",
      "Processed 2500 examples. acc: 0.2816, acc_norm: 0.2700\n",
      "Processed 2600 examples. acc: 0.2800, acc_norm: 0.2688\n",
      "Processed 2700 examples. acc: 0.2793, acc_norm: 0.2685\n",
      "Processed 2800 examples. acc: 0.2836, acc_norm: 0.2675\n",
      "Processed 2900 examples. acc: 0.2807, acc_norm: 0.2669\n",
      "Processed 3000 examples. acc: 0.2853, acc_norm: 0.2683\n",
      "Processed 3100 examples. acc: 0.2868, acc_norm: 0.2674\n",
      "Processed 3200 examples. acc: 0.2878, acc_norm: 0.2687\n",
      "Processed 3300 examples. acc: 0.2855, acc_norm: 0.2697\n",
      "Processed 3400 examples. acc: 0.2850, acc_norm: 0.2691\n",
      "Processed 3500 examples. acc: 0.2823, acc_norm: 0.2689\n",
      "Processed 3600 examples. acc: 0.2825, acc_norm: 0.2700\n",
      "Processed 3700 examples. acc: 0.2814, acc_norm: 0.2692\n",
      "Processed 3800 examples. acc: 0.2818, acc_norm: 0.2689\n",
      "Processed 3900 examples. acc: 0.2803, acc_norm: 0.2690\n",
      "Processed 4000 examples. acc: 0.2810, acc_norm: 0.2710\n",
      "Processed 4100 examples. acc: 0.2800, acc_norm: 0.2715\n",
      "Processed 4200 examples. acc: 0.2810, acc_norm: 0.2698\n",
      "Processed 4300 examples. acc: 0.2807, acc_norm: 0.2707\n",
      "Processed 4400 examples. acc: 0.2802, acc_norm: 0.2695\n",
      "Processed 4500 examples. acc: 0.2807, acc_norm: 0.2704\n",
      "Processed 4600 examples. acc: 0.2798, acc_norm: 0.2709\n",
      "Processed 4700 examples. acc: 0.2783, acc_norm: 0.2698\n",
      "Processed 4800 examples. acc: 0.2775, acc_norm: 0.2710\n",
      "Processed 4900 examples. acc: 0.2778, acc_norm: 0.2704\n",
      "Processed 5000 examples. acc: 0.2784, acc_norm: 0.2690\n",
      "Processed 5100 examples. acc: 0.2782, acc_norm: 0.2694\n",
      "Processed 5200 examples. acc: 0.2785, acc_norm: 0.2690\n",
      "Processed 5300 examples. acc: 0.2781, acc_norm: 0.2696\n",
      "Processed 5400 examples. acc: 0.2780, acc_norm: 0.2691\n",
      "Processed 5500 examples. acc: 0.2765, acc_norm: 0.2691\n",
      "Processed 5600 examples. acc: 0.2755, acc_norm: 0.2686\n",
      "Processed 5700 examples. acc: 0.2744, acc_norm: 0.2686\n",
      "Processed 5800 examples. acc: 0.2740, acc_norm: 0.2693\n",
      "Processed 5900 examples. acc: 0.2734, acc_norm: 0.2683\n",
      "Processed 6000 examples. acc: 0.2738, acc_norm: 0.2680\n",
      "Processed 6100 examples. acc: 0.2734, acc_norm: 0.2677\n",
      "Processed 6200 examples. acc: 0.2734, acc_norm: 0.2669\n",
      "Processed 6300 examples. acc: 0.2724, acc_norm: 0.2678\n",
      "Processed 6400 examples. acc: 0.2716, acc_norm: 0.2670\n",
      "Processed 6500 examples. acc: 0.2717, acc_norm: 0.2675\n",
      "Processed 6600 examples. acc: 0.2708, acc_norm: 0.2667\n",
      "Processed 6700 examples. acc: 0.2699, acc_norm: 0.2675\n",
      "Processed 6800 examples. acc: 0.2687, acc_norm: 0.2669\n",
      "Processed 6900 examples. acc: 0.2691, acc_norm: 0.2675\n",
      "Processed 7000 examples. acc: 0.2691, acc_norm: 0.2681\n",
      "Processed 7100 examples. acc: 0.2690, acc_norm: 0.2682\n",
      "Processed 7200 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7300 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7400 examples. acc: 0.2685, acc_norm: 0.2688\n",
      "Processed 7500 examples. acc: 0.2684, acc_norm: 0.2687\n",
      "Processed 7600 examples. acc: 0.2680, acc_norm: 0.2675\n",
      "Processed 7700 examples. acc: 0.2666, acc_norm: 0.2670\n",
      "Processed 7800 examples. acc: 0.2662, acc_norm: 0.2668\n",
      "Processed 7900 examples. acc: 0.2659, acc_norm: 0.2665\n",
      "Processed 8000 examples. acc: 0.2661, acc_norm: 0.2661\n",
      "Processed 8100 examples. acc: 0.2653, acc_norm: 0.2654\n",
      "Processed 8200 examples. acc: 0.2648, acc_norm: 0.2659\n",
      "Processed 8300 examples. acc: 0.2640, acc_norm: 0.2648\n",
      "Processed 8400 examples. acc: 0.2639, acc_norm: 0.2644\n",
      "Processed 8500 examples. acc: 0.2631, acc_norm: 0.2642\n",
      "Processed 8600 examples. acc: 0.2623, acc_norm: 0.2637\n",
      "Processed 8700 examples. acc: 0.2617, acc_norm: 0.2639\n",
      "Processed 8800 examples. acc: 0.2617, acc_norm: 0.2642\n",
      "Processed 8900 examples. acc: 0.2612, acc_norm: 0.2639\n",
      "Processed 9000 examples. acc: 0.2611, acc_norm: 0.2634\n",
      "Processed 9100 examples. acc: 0.2619, acc_norm: 0.2634\n",
      "Processed 9200 examples. acc: 0.2617, acc_norm: 0.2635\n",
      "Processed 9300 examples. acc: 0.2614, acc_norm: 0.2635\n",
      "Processed 9400 examples. acc: 0.2610, acc_norm: 0.2638\n",
      "Processed 9500 examples. acc: 0.2612, acc_norm: 0.2636\n",
      "Processed 9600 examples. acc: 0.2607, acc_norm: 0.2635\n",
      "Processed 9700 examples. acc: 0.2606, acc_norm: 0.2638\n",
      "Processed 9800 examples. acc: 0.2599, acc_norm: 0.2636\n",
      "Processed 9900 examples. acc: 0.2600, acc_norm: 0.2639\n",
      "Processed 10000 examples. acc: 0.2593, acc_norm: 0.2636\n",
      "Done! Evaluated 10042 examples.\n",
      "Final Acc: 0.2594  Acc_norm: 0.2634\n"
     ]
    }
   ],
   "source": [
    "evaluate_iGPT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sd_hf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msd_hf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.wpe.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sd_hf' is not defined"
     ]
    }
   ],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instead sample manually\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000] # first 1,000 characters\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf[\"lm_head.weight\"].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sd_hf[\"lm_head.weight\"] == sd_hf[\"transformer.wte.weight\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf[\"lm_head.weight\"].data_ptr())\n",
    "print(sd_hf[\"transformer.wte.weight\"].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standard deviation grows inside the residual stream\n",
    "x = torch.zeros(768)\n",
    "n = 100 # e.g. 100 layers\n",
    "for i in range(n):\n",
    "    x += n**-0.5 * torch.randn(768)\n",
    "\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# super simple little MLP\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1)\n",
    ")\n",
    "torch.random.manual_seed(42)\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "# the loss objective here is (due to readuction='mean')\n",
    "# L = 1/4 * [\n",
    "#            (y[0] - yhat[0])**2 +\n",
    "#            (y[1] - yhat[1])**2 +\n",
    "#            (y[2] - yhat[2])**2 +\n",
    "#            (y[3] - yhat[3])**2\n",
    "#           ]\n",
    "# NOTE: 1/4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do it with grad_accum_steps of 4, and B=1\n",
    "# the loss objective here is different because\n",
    "# accumulation in gradient <---> SUM in loss\n",
    "# i.e. we instead get:\n",
    "# L0 = 1/4(y[0] - yhat[0])**2\n",
    "# L1 = 1/4(y[1] - yhat[1])**2\n",
    "# L2 = 1/4(y[2] - yhat[2])**2\n",
    "# L3 = 1/4(y[3] - yhat[3])**2\n",
    "# L = L0 + L1 + L2 + L3\n",
    "# NOTE: the \"normalizer\" of 1/4 is lost\n",
    "net.zero_grad()\n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss = loss / 4 # <-- have to add back the \"normalizer\"!\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and visualize the logfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log124M_40B/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
