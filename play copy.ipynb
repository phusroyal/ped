{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phusr/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotMyModel(\n",
       "  (network): iGPT(\n",
       "    (wte_i): Embedding(50304, 512)\n",
       "    (wpe_i): Embedding(256, 512)\n",
       "    (blocks_i): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (idea_head): Linear(in_features=512, out_features=768, bias=False)\n",
       "    (wte_g): Embedding(50304, 768)\n",
       "    (wpe_g): Embedding(256, 768)\n",
       "    (blocks_g): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_g): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.iGPT import iGPT,iGPTConfig, NotMyModel\n",
    "import torch\n",
    "\n",
    "config = iGPTConfig(\n",
    "    block_size=256,\n",
    "    vocab_size=50304,\n",
    "    n_layer_main=12,\n",
    "    n_layer_idea=12,\n",
    "    n_head=8,\n",
    "    n_embd_main = 768,          \n",
    "    n_embd_idea = 512,\n",
    "    idea_dim=768,\n",
    ")\n",
    "\n",
    "# model = NotMyModel(config)\n",
    "model = NotMyModel.load_from_checkpoint(\"experiments/lightning_logs/version_0/checkpoints/epoch=5-step=14149.ckpt\")\n",
    "# model.cuda()\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# Load the base encoding\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eos = 50257\n",
    "# Define new special tokens\n",
    "new_special_tokens = {\n",
    "    \"<|endofsent|>\": eos,  # Make sure this ID does not conflict with existing tokens\n",
    "}\n",
    "# Create a new encoding with the added special tokens\n",
    "extended_enc = tiktoken.Encoding(\n",
    "    name=\"gpt2_extended\",\n",
    "    pat_str=enc._pat_str,  # Use the same pattern as the original encoding\n",
    "    mergeable_ranks=enc._mergeable_ranks,  # Keep the same mergeable ranks\n",
    "    special_tokens={**enc._special_tokens, **new_special_tokens},  # Extend special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 50\n",
    "# mysent = \"He is credited as a writer on Halo 3 .\"\n",
    "mysent = \"Among the places he has lived since retirement are California and Grants Pass , Oregon .\"\n",
    "sent = extended_enc.encode(mysent)\n",
    "sent = torch.tensor(sent, dtype=torch.long) # (8,)\n",
    "sent = sent.unsqueeze(0)\n",
    "sent = sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_sent = extended_enc.encode(\"<|endofsent|>\", allowed_special={'<|endofsent|>'})\n",
    "init_sent = torch.tensor(init_sent, dtype=torch.long) # (8,)\n",
    "init_sent = init_sent.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "init_sent = init_sent.cuda()\n",
    "init_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "while init_sent.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    # with torch.no_grad():\n",
    "        logits = model((init_sent, sent)) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        init_sent = torch.cat((init_sent, xcol), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among the places he has lived since retirement are California and Grants Pass , Oregon .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <|endofsent|>For the season in two years as a backup actress to appear between April 31 , 2015 through mid @-@ season episodes , with multiple DVDs and low @-@ ratings shows .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issues it became popular with a new brand including limited exclusive online provider ( EP sales ) , its retail @-@ based content and quality sales amounted to 2 @.<|endofsent|>.<|endofsent|><|endofsent|><|endofsent|>.<|endofsent|><|endofsent|>.<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issue he started work and would be shown at various social organizations including : Atlanta University ( where Denny is mentioned as <unk> ) and Austin Cooder University .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the season he was also a substitute ; former NHL champion Du Bois appeared in mid @-@ September , leading him with 10 assists and 20 assists and 15 doubles .<|endofsent|>gers .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the week it became very popular for artists across Brazil , from several poets , and musicians including the artists , ethnographers J. Mait , and many Canadian musicians supported worldwide .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n"
     ]
    }
   ],
   "source": [
    "# small models (4 layers decoder, 2 layer encoder) - 4 epochs\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = init_sent[i, :max_length].tolist()\n",
    "    decoded = extended_enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# lightning module\n",
    "from model.iGPT import NotMyModel  # import your LightningModule class\n",
    "# same code for HellaSwag download + rendering as in your GPT-2 script\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(\"data\"), \"hellaswag\")\n",
    "hellaswags = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "}\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def download_file(url: str, fname: str, chunk_size=1024):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(fname, \"wb\") as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit=\"iB\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def download(split):\n",
    "    \"\"\"Downloads HellaSwag into DATA_CACHE_DIR if not already present.\"\"\"\n",
    "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "    data_url = hellaswags[split]\n",
    "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "    if not os.path.exists(data_filename):\n",
    "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "        download_file(data_url, data_filename)\n",
    "\n",
    "def iterate_examples(split):\n",
    "    download(split)\n",
    "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "\n",
    "def render_example(example):\n",
    "    \"\"\"Same as your GPT-2 code, returns (data, tokens, mask, label).\"\"\"\n",
    "    ctx = example[\"ctx\"]\n",
    "    label = example[\"label\"]\n",
    "    endings = example[\"endings\"]\n",
    "\n",
    "    data = {\n",
    "        \"label\": label,\n",
    "        \"ctx_tokens\": None,\n",
    "        \"ending_tokens\": [],\n",
    "    }\n",
    "    # Tokenize context\n",
    "    ctx_tokens = enc.encode(ctx)\n",
    "    data[\"ctx_tokens\"] = ctx_tokens\n",
    "\n",
    "    # Tokenize each of the 4 endings\n",
    "    tok_rows = []\n",
    "    mask_rows = []\n",
    "    for end in endings:\n",
    "        end_tokens = enc.encode(\" \" + end)  # note: leading space\n",
    "        tok_rows.append(ctx_tokens + end_tokens)\n",
    "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "        data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "    # Collate into a 4 x max_len shape\n",
    "    max_len = max(len(row) for row in tok_rows)\n",
    "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "    return data, tokens, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_iGPT(model_module, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Loads your trained iGPT model from a Lightning checkpoint, then\n",
    "    evaluates on the HellaSwag val set. We measure two scores:\n",
    "\n",
    "    1) 'acc' using sum of cross-entropy loss over each candidate (pred = argmin sum).\n",
    "    2) 'acc_norm' using average cross-entropy over each candidate region (pred = argmin mean).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # For convenience, we'll directly call model_module.network() \n",
    "    # inside our loop below. If your model requires anything special \n",
    "    # for generation or inference, adapt as needed.\n",
    "\n",
    "    # 2) Setup counters\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # 3) Iterate over HellaSwag val examples\n",
    "    for example in iterate_examples(\"val\"):\n",
    "        data, tokens, mask, label = render_example(example)\n",
    "        tokens = tokens.to(device)  # shape: (4, max_len)\n",
    "        mask = mask.to(device)      # shape: (4, max_len)\n",
    "\n",
    "        # iGPT expects two inputs: x, ix\n",
    "        # We'll feed 'tokens' as x, and supply a dummy 'ix' \n",
    "        # because we don't have any \"idea tokens\" at test time.\n",
    "        B, T = tokens.size()  # B = 4\n",
    "        ix_dummy = torch.zeros((B, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass to get logits: shape (B, T, vocab_size)\n",
    "        logits = model_module((tokens, ix_dummy))\n",
    "\n",
    "        # 4) Autoregressive loss\n",
    "        # same procedure as the GPT-2 script\n",
    "        shift_logits = logits[..., :-1, :].contiguous()   # (4, T-1, vocab_size)\n",
    "        shift_tokens = tokens[..., 1:].contiguous()       # (4, T-1)\n",
    "        flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "        flat_shift_tokens = shift_tokens.view(-1)\n",
    "        shift_losses = F.cross_entropy(\n",
    "            flat_shift_logits, \n",
    "            flat_shift_tokens,\n",
    "            reduction='none'\n",
    "        )\n",
    "        shift_losses = shift_losses.view(B, -1)  # shape (4, T-1)\n",
    "\n",
    "        # apply the mask\n",
    "        shift_mask = mask[..., 1:].contiguous()  # also shape (4, T-1)\n",
    "        masked_shift_losses = shift_losses * shift_mask\n",
    "\n",
    "        # sum/average over the candidate region\n",
    "        sum_loss = masked_shift_losses.sum(dim=1)          # shape (4,)\n",
    "        avg_loss = sum_loss / shift_mask.sum(dim=1)        # shape (4,)\n",
    "        \n",
    "        # the best candidate is the one with the lowest sum_loss\n",
    "        pred = sum_loss.argmin().item()\n",
    "        pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "        # 5) Accumulate stats\n",
    "        num_total += 1\n",
    "        num_correct += int(pred == label)\n",
    "        num_correct_norm += int(pred_norm == label)\n",
    "\n",
    "        # (Optional) debug prints\n",
    "        # if num_total <= 10:\n",
    "        #     print(\"---\")\n",
    "        #     print(f\"Context:\\n {example['ctx']}\")\n",
    "        #     print(\"Endings:\")\n",
    "        #     for i, end in enumerate(example[\"endings\"]):\n",
    "        #         print(f\"{i} (loss: {avg_loss[i].item():.4f}) => {end}\")\n",
    "        #     print(f\"Predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "        # (Optional) show running accuracy\n",
    "        if num_total % 100 == 0:\n",
    "            print(f\"Processed {num_total} examples. \" \n",
    "                  f\"acc: {num_correct/num_total:.4f}, \"\n",
    "                  f\"acc_norm: {num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "    # 6) Final results\n",
    "    print(f\"Done! Evaluated {num_total} examples.\")\n",
    "    acc = num_correct/num_total\n",
    "    acc_norm = num_correct_norm/num_total\n",
    "    print(f\"Final Acc: {acc:.4f}  Acc_norm: {acc_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples. acc: 0.2500, acc_norm: 0.1800\n",
      "Processed 200 examples. acc: 0.2750, acc_norm: 0.2450\n",
      "Processed 300 examples. acc: 0.2867, acc_norm: 0.2667\n",
      "Processed 400 examples. acc: 0.2650, acc_norm: 0.2725\n",
      "Processed 500 examples. acc: 0.2640, acc_norm: 0.2680\n",
      "Processed 600 examples. acc: 0.2950, acc_norm: 0.2867\n",
      "Processed 700 examples. acc: 0.2871, acc_norm: 0.2800\n",
      "Processed 800 examples. acc: 0.2750, acc_norm: 0.2900\n",
      "Processed 900 examples. acc: 0.2756, acc_norm: 0.2967\n",
      "Processed 1000 examples. acc: 0.2720, acc_norm: 0.2910\n",
      "Processed 1100 examples. acc: 0.2682, acc_norm: 0.2882\n",
      "Processed 1200 examples. acc: 0.2717, acc_norm: 0.2850\n",
      "Processed 1300 examples. acc: 0.2746, acc_norm: 0.2815\n",
      "Processed 1400 examples. acc: 0.2729, acc_norm: 0.2800\n",
      "Processed 1500 examples. acc: 0.2653, acc_norm: 0.2733\n",
      "Processed 1600 examples. acc: 0.2637, acc_norm: 0.2669\n",
      "Processed 1700 examples. acc: 0.2718, acc_norm: 0.2641\n",
      "Processed 1800 examples. acc: 0.2756, acc_norm: 0.2650\n",
      "Processed 1900 examples. acc: 0.2711, acc_norm: 0.2642\n",
      "Processed 2000 examples. acc: 0.2730, acc_norm: 0.2640\n",
      "Processed 2100 examples. acc: 0.2714, acc_norm: 0.2690\n",
      "Processed 2200 examples. acc: 0.2736, acc_norm: 0.2677\n",
      "Processed 2300 examples. acc: 0.2748, acc_norm: 0.2696\n",
      "Processed 2400 examples. acc: 0.2812, acc_norm: 0.2700\n",
      "Processed 2500 examples. acc: 0.2816, acc_norm: 0.2700\n",
      "Processed 2600 examples. acc: 0.2800, acc_norm: 0.2688\n",
      "Processed 2700 examples. acc: 0.2793, acc_norm: 0.2685\n",
      "Processed 2800 examples. acc: 0.2836, acc_norm: 0.2675\n",
      "Processed 2900 examples. acc: 0.2807, acc_norm: 0.2669\n",
      "Processed 3000 examples. acc: 0.2853, acc_norm: 0.2683\n",
      "Processed 3100 examples. acc: 0.2868, acc_norm: 0.2674\n",
      "Processed 3200 examples. acc: 0.2878, acc_norm: 0.2687\n",
      "Processed 3300 examples. acc: 0.2855, acc_norm: 0.2697\n",
      "Processed 3400 examples. acc: 0.2850, acc_norm: 0.2691\n",
      "Processed 3500 examples. acc: 0.2823, acc_norm: 0.2689\n",
      "Processed 3600 examples. acc: 0.2825, acc_norm: 0.2700\n",
      "Processed 3700 examples. acc: 0.2814, acc_norm: 0.2692\n",
      "Processed 3800 examples. acc: 0.2818, acc_norm: 0.2689\n",
      "Processed 3900 examples. acc: 0.2803, acc_norm: 0.2690\n",
      "Processed 4000 examples. acc: 0.2810, acc_norm: 0.2710\n",
      "Processed 4100 examples. acc: 0.2800, acc_norm: 0.2715\n",
      "Processed 4200 examples. acc: 0.2810, acc_norm: 0.2698\n",
      "Processed 4300 examples. acc: 0.2807, acc_norm: 0.2707\n",
      "Processed 4400 examples. acc: 0.2802, acc_norm: 0.2695\n",
      "Processed 4500 examples. acc: 0.2807, acc_norm: 0.2704\n",
      "Processed 4600 examples. acc: 0.2798, acc_norm: 0.2709\n",
      "Processed 4700 examples. acc: 0.2783, acc_norm: 0.2698\n",
      "Processed 4800 examples. acc: 0.2775, acc_norm: 0.2710\n",
      "Processed 4900 examples. acc: 0.2778, acc_norm: 0.2704\n",
      "Processed 5000 examples. acc: 0.2784, acc_norm: 0.2690\n",
      "Processed 5100 examples. acc: 0.2782, acc_norm: 0.2694\n",
      "Processed 5200 examples. acc: 0.2785, acc_norm: 0.2690\n",
      "Processed 5300 examples. acc: 0.2781, acc_norm: 0.2696\n",
      "Processed 5400 examples. acc: 0.2780, acc_norm: 0.2691\n",
      "Processed 5500 examples. acc: 0.2765, acc_norm: 0.2691\n",
      "Processed 5600 examples. acc: 0.2755, acc_norm: 0.2686\n",
      "Processed 5700 examples. acc: 0.2744, acc_norm: 0.2686\n",
      "Processed 5800 examples. acc: 0.2740, acc_norm: 0.2693\n",
      "Processed 5900 examples. acc: 0.2734, acc_norm: 0.2683\n",
      "Processed 6000 examples. acc: 0.2738, acc_norm: 0.2680\n",
      "Processed 6100 examples. acc: 0.2734, acc_norm: 0.2677\n",
      "Processed 6200 examples. acc: 0.2734, acc_norm: 0.2669\n",
      "Processed 6300 examples. acc: 0.2724, acc_norm: 0.2678\n",
      "Processed 6400 examples. acc: 0.2716, acc_norm: 0.2670\n",
      "Processed 6500 examples. acc: 0.2717, acc_norm: 0.2675\n",
      "Processed 6600 examples. acc: 0.2708, acc_norm: 0.2667\n",
      "Processed 6700 examples. acc: 0.2699, acc_norm: 0.2675\n",
      "Processed 6800 examples. acc: 0.2687, acc_norm: 0.2669\n",
      "Processed 6900 examples. acc: 0.2691, acc_norm: 0.2675\n",
      "Processed 7000 examples. acc: 0.2691, acc_norm: 0.2681\n",
      "Processed 7100 examples. acc: 0.2690, acc_norm: 0.2682\n",
      "Processed 7200 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7300 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7400 examples. acc: 0.2685, acc_norm: 0.2688\n",
      "Processed 7500 examples. acc: 0.2684, acc_norm: 0.2687\n",
      "Processed 7600 examples. acc: 0.2680, acc_norm: 0.2675\n",
      "Processed 7700 examples. acc: 0.2666, acc_norm: 0.2670\n",
      "Processed 7800 examples. acc: 0.2662, acc_norm: 0.2668\n",
      "Processed 7900 examples. acc: 0.2659, acc_norm: 0.2665\n",
      "Processed 8000 examples. acc: 0.2661, acc_norm: 0.2661\n",
      "Processed 8100 examples. acc: 0.2653, acc_norm: 0.2654\n",
      "Processed 8200 examples. acc: 0.2648, acc_norm: 0.2659\n",
      "Processed 8300 examples. acc: 0.2640, acc_norm: 0.2648\n",
      "Processed 8400 examples. acc: 0.2639, acc_norm: 0.2644\n",
      "Processed 8500 examples. acc: 0.2631, acc_norm: 0.2642\n",
      "Processed 8600 examples. acc: 0.2623, acc_norm: 0.2637\n",
      "Processed 8700 examples. acc: 0.2617, acc_norm: 0.2639\n",
      "Processed 8800 examples. acc: 0.2617, acc_norm: 0.2642\n",
      "Processed 8900 examples. acc: 0.2612, acc_norm: 0.2639\n",
      "Processed 9000 examples. acc: 0.2611, acc_norm: 0.2634\n",
      "Processed 9100 examples. acc: 0.2619, acc_norm: 0.2634\n",
      "Processed 9200 examples. acc: 0.2617, acc_norm: 0.2635\n",
      "Processed 9300 examples. acc: 0.2614, acc_norm: 0.2635\n",
      "Processed 9400 examples. acc: 0.2610, acc_norm: 0.2638\n",
      "Processed 9500 examples. acc: 0.2612, acc_norm: 0.2636\n",
      "Processed 9600 examples. acc: 0.2607, acc_norm: 0.2635\n",
      "Processed 9700 examples. acc: 0.2606, acc_norm: 0.2638\n",
      "Processed 9800 examples. acc: 0.2599, acc_norm: 0.2636\n",
      "Processed 9900 examples. acc: 0.2600, acc_norm: 0.2639\n",
      "Processed 10000 examples. acc: 0.2593, acc_norm: 0.2636\n",
      "Done! Evaluated 10042 examples.\n",
      "Final Acc: 0.2594  Acc_norm: 0.2634\n"
     ]
    }
   ],
   "source": [
    "evaluate_iGPT(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dcbc56aba04b8f80599bbe5f4853df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch=3-step=83166.ckpt:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a4ad8b17214360a9158b8aa516d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch=3-step=83266.ckpt:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e75e7b44fc5486ba263b551352d42b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737308482.trustworthy-mbzuai.3253248.0:   0%|          | 0.00/26.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45b7a1ec6a49d28258c1fcbc1950b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"phusroyal/my-lightning-checkpoints\"\n",
    "folder_path = \"experiments/lightning_logs/version_010\"  # wherever your files are\n",
    "\n",
    "# If you haven't created the repo yet:\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Upload all files in `folder_path` to the Hugging Face Hub\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=folder_path,\n",
    "    commit_message=\"Upload igptv010\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phusr/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotMyModel(\n",
       "  (network): iGPT(\n",
       "    (wte_i): Embedding(50304, 512)\n",
       "    (wpe_i): Embedding(256, 512)\n",
       "    (blocks_i): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (idea_head): Linear(in_features=512, out_features=768, bias=False)\n",
       "    (wte_g): Embedding(50304, 768)\n",
       "    (wpe_g): Embedding(256, 768)\n",
       "    (blocks_g): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_g): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from model.iGPT import NotMyModel  # your custom LightningModule\n",
    "\n",
    "repo_id = \"phusroyal/my-lightning-checkpoints\"\n",
    "# Adjust the filename if your checkpoint file has a different name\n",
    "filename = \"checkpoints/epoch=4-step=13604.ckpt\"\n",
    "\n",
    "# This downloads the file and returns the local path\n",
    "ckpt_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Now load it in your LightningModule\n",
    "model_pl = NotMyModel.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "# Put model in eval mode, move to GPU/CPU, etc.\n",
    "model_pl.eval()\n",
    "model_pl.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import lightning as L\n",
    "from dataclasses import dataclass\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from lightning.pytorch.utilities import grad_norm\n",
    "\n",
    "# from model.transformer_blocks import Block, CausalSelfAttention, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)                   # (B, T, 3C)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embd, mlp_ratio * n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(mlp_ratio * n_embd, n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # removed the 'idea_vector' argument / logic\n",
    "    def __init__(self, n_embd, n_head, mlp_ratio=4.0, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head)\n",
    "        self.ln_2 = norm_layer(n_embd)\n",
    "        self.mlp = MLP(n_embd, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedIdearEncoder(nn.Module):\n",
    "    def __init__(self, st_model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        # Load the Sentence Transformer model\n",
    "        self.sbert = SentenceTransformer(st_model_name)\n",
    "        # The output dimension is typically 768\n",
    "        self.sbert_dim = 768\n",
    "\n",
    "    def forward(self, sentence_list):\n",
    "        # Get the idea embeddings from all-mpnet-base-v2\n",
    "        with torch.no_grad():\n",
    "            idea_vecs = self.sbert.encode(sentence_list, convert_to_tensor=True)  # shape (B, 768)\n",
    "        return idea_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaPredictor(nn.Module):\n",
    "    def __init__(self, \n",
    "                block_size=32,\n",
    "                input_dim=768,\n",
    "                predictor_embed_dim=384,\n",
    "                predictor_depth=12,\n",
    "                predictor_num_heads=12,\n",
    "                mlp_ratio=4.0,\n",
    "                # qkv_bias=False,\n",
    "                # qk_scale=None,\n",
    "                # drop_rate=0.0,\n",
    "                # attn_drop_rate=0.0,\n",
    "                # drop_path_rate=0.0,\n",
    "                # init_std=0.02,\n",
    "                norm_layer=nn.LayerNorm,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize the IdeaPredictor model.\n",
    "        Args:\n",
    "            block_size (int): The maximum number of sequences for the predictor.\n",
    "            input_dim (int): The dimension of the input idea embeddings.\n",
    "            predictor_embed_dim (int): The embedding dimension for the predictor.\n",
    "            predictor_depth (int): The number of transformer blocks in the predictor.\n",
    "            predictor_num_heads (int): The number of attention heads in each transformer block.\n",
    "            mlp_ratio (float): The ratio of the hidden dimension to the embedding dimension in the MLP.\n",
    "            norm_layer (nn.Module): The normalization layer to use.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.input_dim = input_dim\n",
    "        self.predictor_embed_dim = predictor_embed_dim\n",
    "        self.predictor_depth = predictor_depth\n",
    "        self.predictor_num_heads = predictor_num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        # Project from idea encoder dimension to predictor dimension\n",
    "        if predictor_embed_dim != input_dim:\n",
    "            self.idea_proj = nn.Linear(input_dim, predictor_embed_dim, bias=False)            \n",
    "            self.predictor_proj = nn.Linear(predictor_embed_dim, input_dim, bias=False)\n",
    "            self.predictor_proj.SCALE_INIT = 1\n",
    "            self.idea_proj.SCALE_INIT = 1\n",
    "        else:\n",
    "            self.idea_proj = nn.Identity()\n",
    "            self.predictor_proj = nn.Identity()\n",
    "\n",
    "        self.predictor_wpe = nn.Embedding(block_size+1, predictor_embed_dim)\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                n_embd=predictor_embed_dim,\n",
    "                n_head=predictor_num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                # qkv_bias=qkv_bias,\n",
    "                # qk_scale=qk_scale,\n",
    "                # drop_rate=drop_rate,\n",
    "                # attn_drop_rate=attn_drop_rate,\n",
    "                # drop_path_rate=drop_path_rate,\n",
    "            )\n",
    "            for _ in range(predictor_depth)\n",
    "        ])\n",
    "\n",
    "        self.predictor_ln = nn.LayerNorm(predictor_embed_dim)\n",
    "\n",
    "        # might need weight tying\n",
    "        # what and why is weight tying?\n",
    "        # what: Weight tying is a technique where the same weight matrix is used for both the input and output projections in a model, reducing the number of parameters and potentially improving generalization.\n",
    "        # why: This can help prevent overfitting, especially in models with large vocabularies or embedding spaces, by forcing the model to learn a more compact representation.\n",
    "        # as idea prediction is a regression task, we do not use weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.predictor_depth) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idea_vecs, target=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the IdeaPredictor model.\n",
    "        Args:\n",
    "            idea_vecs (torch.Tensor): Input idea embeddings of shape (B, I, input_dim), where B is the batch size and I is the number of ideas.\n",
    "            target (torch.Tensor, optional): Target values for training. Not used in this implementation.\n",
    "        \"\"\"\n",
    "        B, I = idea_vecs.size(0), idea_vecs.size(1)\n",
    "\n",
    "        # Project the idea embeddings\n",
    "        idea_emb = self.idea_proj(idea_vecs)  # shape (B, I, predictor_embed_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        pos_ids = torch.arange(0, I, device=idea_vecs.device)  # length T\n",
    "        pos_emb = self.predictor_wpe(pos_ids)  # shape  (B, I, predictor_embed_dim)\n",
    "        hidden = idea_emb + pos_emb  # (B, I, predictor_embed_dim)\n",
    "\n",
    "        # Forward through predictor blocks\n",
    "        for block in self.predictor_blocks:\n",
    "            hidden = block(hidden)\n",
    "        hidden = self.predictor_ln(hidden)\n",
    "\n",
    "        # Project back to the original dimension\n",
    "        hidden = self.predictor_proj(hidden)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                block_size=32,\n",
    "                vocab_size=50304,\n",
    "                input_dim=768,\n",
    "                decoder_embed_dim=384,\n",
    "                decoder_depth=6,\n",
    "                decoder_num_heads=12,\n",
    "                mlp_ratio=4.0,\n",
    "                # qkv_bias=False,\n",
    "                # qk_scale=None,\n",
    "                # drop_rate=0.0,\n",
    "                # attn_drop_rate=0.0,\n",
    "                # drop_path_rate=0.0,\n",
    "                # init_std=0.02,\n",
    "                norm_layer=nn.LayerNorm,\n",
    "                ):\n",
    "        \"\"\" Initialize the IdeaDecoder model.\n",
    "        Args:\n",
    "            block_size (int): The maximum number of tokens for the decoder.\n",
    "            vocab_size (int): The size of the vocabulary for the decoder.\n",
    "            input_dim (int): The dimension of the input idea embeddings.\n",
    "            decoder_embed_dim (int): The embedding dimension for the decoder.\n",
    "            decoder_depth (int): The number of transformer blocks in the decoder.\n",
    "            decoder_num_heads (int): The number of attention heads in each transformer block.\n",
    "            mlp_ratio (float): The ratio of the hidden dimension to the embedding dimension in the MLP.\n",
    "            norm_layer (nn.Module): The normalization layer to use.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.input_dim = input_dim\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_depth = decoder_depth\n",
    "        self.decoder_num_heads = decoder_num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm_layer = norm_layer\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Project from idea encoder dimension to predictor dimension\n",
    "        if decoder_embed_dim != input_dim:\n",
    "            self.idea_proj = nn.Linear(input_dim, decoder_embed_dim, bias=False)            \n",
    "            self.decoder_proj = nn.Linear(decoder_embed_dim, input_dim, bias=False)\n",
    "            self.decoder_proj.SCALE_INIT = 1\n",
    "            self.idea_proj.SCALE_INIT = 1\n",
    "        else:\n",
    "            self.idea_proj = nn.Identity()\n",
    "            self.decoder_proj = nn.Identity()\n",
    "\n",
    "        self.decoder_wte = nn.Embedding(vocab_size, decoder_embed_dim)\n",
    "        self.decoder_wpe = nn.Embedding(block_size+1, decoder_embed_dim)\n",
    "        self.block = nn.ModuleList([\n",
    "            Block(\n",
    "                n_embd=decoder_embed_dim,\n",
    "                n_head=decoder_num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                # qkv_bias=qkv_bias,\n",
    "                # qk_scale=qk_scale,\n",
    "                # drop_rate=drop_rate,\n",
    "                # attn_drop_rate=attn_drop_rate,\n",
    "                # drop_path_rate=drop_path_rate,\n",
    "            )\n",
    "            for _ in range(decoder_depth)\n",
    "        ])\n",
    "\n",
    "        self.decoder_ln = nn.LayerNorm(decoder_embed_dim)\n",
    "        self.decoder_lm_head = nn.Linear(decoder_embed_dim, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.decoder_wte.weight = self.decoder_lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.decoder_depth) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, idea_vec, target=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the IdeaDecoder model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token indices of shape (B*I, T), where B is the batch size and I is the number of ideas.\n",
    "            idea_vec (torch.Tensor): Input idea embeddings of shape (B*I, input_dim).\n",
    "            target (torch.Tensor, optional): Target values for training. Not used in this implementation.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits for next-word prediction of shape (B*I, T, vocab_size).   \n",
    "        \"\"\"\n",
    "        B, T = x.size()   # now B and T are ints\n",
    "\n",
    "        # Project the idea embeddings\n",
    "        idea_vec = self.idea_proj(idea_vec)\n",
    "\n",
    "        # forward the token and positional embeddings\n",
    "        token_emb = self.decoder_wte(x)  # (B*I, T, decoder_embed_dim)\n",
    "        pos_ids = torch.arange(0, T, device=x.device)\n",
    "        pos_emb = self.decoder_wpe(pos_ids)       \n",
    "        idea_token = idea_vec.unsqueeze(1)                 # (B*I, 1, D)\n",
    "        token_and_pos = token_emb + pos_emb               # (B*I, T, D)\n",
    "        hidden = torch.cat([idea_token, token_and_pos], 1) # (B*I, T+1, D)\n",
    "\n",
    "\n",
    "        # Forward through decoder blocks\n",
    "        for block in self.block:\n",
    "            hidden = block(hidden)\n",
    "\n",
    "        # Normalize the hidden states\n",
    "        hidden = self.decoder_ln(hidden)\n",
    "\n",
    "        # skip the idea token when producing logits for next-word prediction\n",
    "        logits = self.decoder_lm_head(hidden[:, 1:, :])  # shape (B*I, T, vocab_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPTConfig:\n",
    "    id_pred_block_size: int = 32\n",
    "    id_pred_n_layer: int = 6\n",
    "    id_pred_n_head: int = 12\n",
    "    id_pred_n_embd: int = 384\n",
    "    id_dec_block_size: int = 32\n",
    "    id_dec_n_layer: int = 6\n",
    "    id_dec_n_head: int = 12\n",
    "    id_dec_n_embd: int = 384\n",
    "    mlp_ratio: float = 4.0\n",
    "    vocab_size: int = 50304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT model that prepends a single 'idea token' derived from\n",
    "    all-mpnet-base-v2 embeddings at the start of the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: iGPTConfig, st_model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # -- idea encoder\n",
    "        # Load the Sentence Transformer model\n",
    "        self.sbert = SentenceTransformer(st_model_name)\n",
    "        self.sbert.eval()  # Set to evaluation mode\n",
    "        self.sbert_dim = self.sbert.get_sentence_embedding_dimension()  # typically 768\n",
    "        \n",
    "        # -- idea predictor\n",
    "        self.idea_predictor = IdeaPredictor(\n",
    "            block_size=config.id_pred_block_size,\n",
    "            input_dim=self.sbert_dim,\n",
    "            predictor_embed_dim=config.id_pred_n_embd,\n",
    "            predictor_depth=config.id_pred_n_layer,\n",
    "            predictor_num_heads=config.id_pred_n_head,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            norm_layer=nn.LayerNorm\n",
    "        )\n",
    "\n",
    "        # -- idea decoder\n",
    "        self.idea_decoder = IdeaDecoder(\n",
    "            block_size=config.id_dec_block_size,\n",
    "            vocab_size=config.vocab_size,\n",
    "            input_dim=self.sbert_dim,\n",
    "            decoder_embed_dim=config.id_dec_n_embd,\n",
    "            decoder_depth=config.id_dec_n_layer,\n",
    "            decoder_num_heads=config.id_dec_n_head,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            norm_layer=nn.LayerNorm\n",
    "        )\n",
    "\n",
    "    def forward(self, xt, xs):\n",
    "        \"\"\"\n",
    "        Forward pass for the iGPT model.\n",
    "        Args:\n",
    "            xt (torch.Tensor): Input token indices of shape (B*I, T), where B is the batch size and I is the number of ideas.\n",
    "            xs (list of str): List of sentences corresponding to each idea in the batch.\n",
    "        Returns:\n",
    "            yt (torch.Tensor): Logits for next-word prediction of shape (B*I, T, vocab_size).\n",
    "            ys (torch.Tensor): Idea embeddings of shape (B*I, predictor_embed_dim).\n",
    "            idea_vecs (torch.Tensor): Idea embeddings from the Sentence Transformer of shape (B, I, 768).\n",
    "        \"\"\"\n",
    "        B, I, T = xt.size(), xt.size(1), xt.size(2)\n",
    "\n",
    "        # Get the idea embeddings from all-mpnet-base-v2 for all sentences I in the batch\n",
    "        with torch.no_grad():\n",
    "            idea_vecs = self.sbert.encode(xs, convert_to_tensor=True, device=xt.device)  # shape (B, I, 768)\n",
    "        \n",
    "        # Pass to idea predictor\n",
    "        ys = self.idea_predictor(idea_vecs) # shape (B, I, predictor_embed_dim)\n",
    "        # flatten to (B*I, predictor_embed_dim)\n",
    "        ys = ys.view(B*I, -1)\n",
    "\n",
    "        # Pass to idea decoder\n",
    "        yt = self.idea_decoder(xt, ys)\n",
    "\n",
    "        return yt, ys, idea_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPTConfig:\n",
    "    id_pred_block_size=2,\n",
    "    id_pred_n_layer=2,      # Reduced from 6\n",
    "    id_pred_n_head=4,       # Reduced from 12\n",
    "    id_pred_n_embd=128,     # Reduced from 384\n",
    "    id_dec_block_size=2,\n",
    "    id_dec_n_layer=2,       # Reduced from 6\n",
    "    id_dec_n_head=4,        # Reduced from 12\n",
    "    id_dec_n_embd=128,      # Reduced from 384\n",
    "    mlp_ratio=4.0,\n",
    "    vocab_size=50304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_igpt():\n",
    "    # Small test configuration\n",
    "    test_config = iGPTConfig()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = iGPT(test_config)\n",
    "    \n",
    "    # Create sample inputs\n",
    "    sample_tokens = torch.randint(0, 50304, (2, 16))  # 2 sequences of length 16\n",
    "    # sample 10  sentences for the idea encoder\n",
    "    sample_sentences =  [\n",
    "        \"This is a sample sentence for idea 1.\",\n",
    "        \"This is another example for idea 2.\",\n",
    "        \"Here is a third idea sentence.\",\n",
    "        \"This is the fourth idea.\",\n",
    "        \"Fifth idea sentence goes here.\",\n",
    "        \"Sixth idea example.\",\n",
    "        \"Seventh idea sentence.\",\n",
    "        \"Eighth idea example.\",\n",
    "        \"Ninth idea sentence.\",\n",
    "        \"Tenth idea example.\"\n",
    "    ]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits, embeddings, _ = model(sample_tokens, sample_sentences)\n",
    "    \n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "    print(f\"Output embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-16-3448240328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_igpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-15-1921598171.py\u001b[0m in \u001b[0;36mtest_igpt\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Create sample inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-14-1455943883.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, st_model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# -- idea predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         self.idea_predictor = IdeaPredictor(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_pred_block_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msbert_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-5-1545234462.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block_size, input_dim, predictor_embed_dim, predictor_depth, predictor_num_heads, mlp_ratio, norm_layer)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Project from idea encoder dimension to predictor dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictor_embed_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midea_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_embed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_embed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCALE_INIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         self.weight = Parameter(\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "test_igpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
