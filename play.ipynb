{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phusr/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotMyModel(\n",
       "  (network): iGPT(\n",
       "    (wte_i): Embedding(50304, 512)\n",
       "    (wpe_i): Embedding(256, 512)\n",
       "    (blocks_i): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (idea_head): Linear(in_features=512, out_features=768, bias=False)\n",
       "    (wte_g): Embedding(50304, 768)\n",
       "    (wpe_g): Embedding(256, 768)\n",
       "    (blocks_g): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_g): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.iGPT import iGPT,iGPTConfig, NotMyModel\n",
    "import torch\n",
    "\n",
    "config = iGPTConfig(\n",
    "    block_size=256,\n",
    "    vocab_size=50304,\n",
    "    n_layer_main=12,\n",
    "    n_layer_idea=12,\n",
    "    n_head=8,\n",
    "    n_embd_main = 768,          \n",
    "    n_embd_idea = 512,\n",
    "    idea_dim=768,\n",
    ")\n",
    "\n",
    "# model = NotMyModel(config)\n",
    "model = NotMyModel.load_from_checkpoint(\"experiments/lightning_logs/version_0/checkpoints/epoch=5-step=14149.ckpt\")\n",
    "# model.cuda()\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# Load the base encoding\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eos = 50257\n",
    "# Define new special tokens\n",
    "new_special_tokens = {\n",
    "    \"<|endofsent|>\": eos,  # Make sure this ID does not conflict with existing tokens\n",
    "}\n",
    "# Create a new encoding with the added special tokens\n",
    "extended_enc = tiktoken.Encoding(\n",
    "    name=\"gpt2_extended\",\n",
    "    pat_str=enc._pat_str,  # Use the same pattern as the original encoding\n",
    "    mergeable_ranks=enc._mergeable_ranks,  # Keep the same mergeable ranks\n",
    "    special_tokens={**enc._special_tokens, **new_special_tokens},  # Extend special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 50\n",
    "# mysent = \"He is credited as a writer on Halo 3 .\"\n",
    "mysent = \"Among the places he has lived since retirement are California and Grants Pass , Oregon .\"\n",
    "sent = extended_enc.encode(mysent)\n",
    "sent = torch.tensor(sent, dtype=torch.long) # (8,)\n",
    "sent = sent.unsqueeze(0)\n",
    "sent = sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257],\n",
       "        [50257]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_sent = extended_enc.encode(\"<|endofsent|>\", allowed_special={'<|endofsent|>'})\n",
    "init_sent = torch.tensor(init_sent, dtype=torch.long) # (8,)\n",
    "init_sent = init_sent.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "init_sent = init_sent.cuda()\n",
    "init_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "while init_sent.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    # with torch.no_grad():\n",
    "        logits = model((init_sent, sent)) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        init_sent = torch.cat((init_sent, xcol), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among the places he has lived since retirement are California and Grants Pass , Oregon .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <|endofsent|>For the season in two years as a backup actress to appear between April 31 , 2015 through mid @-@ season episodes , with multiple DVDs and low @-@ ratings shows .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issues it became popular with a new brand including limited exclusive online provider ( EP sales ) , its retail @-@ based content and quality sales amounted to 2 @.<|endofsent|>.<|endofsent|><|endofsent|><|endofsent|>.<|endofsent|><|endofsent|>.<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>Despite the issue he started work and would be shown at various social organizations including : Atlanta University ( where Denny is mentioned as <unk> ) and Austin Cooder University .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the season he was also a substitute ; former NHL champion Du Bois appeared in mid @-@ September , leading him with 10 assists and 20 assists and 15 doubles .<|endofsent|>gers .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n",
      "> <|endofsent|>For the week it became very popular for artists across Brazil , from several poets , and musicians including the artists , ethnographers J. Mait , and many Canadian musicians supported worldwide .<|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|><|endofsent|>\n"
     ]
    }
   ],
   "source": [
    "# small models (4 layers decoder, 2 layer encoder) - 4 epochs\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = init_sent[i, :max_length].tolist()\n",
    "    decoded = extended_enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# lightning module\n",
    "from model.iGPT import NotMyModel  # import your LightningModule class\n",
    "# same code for HellaSwag download + rendering as in your GPT-2 script\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(\"data\"), \"hellaswag\")\n",
    "hellaswags = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "}\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def download_file(url: str, fname: str, chunk_size=1024):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(fname, \"wb\") as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit=\"iB\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def download(split):\n",
    "    \"\"\"Downloads HellaSwag into DATA_CACHE_DIR if not already present.\"\"\"\n",
    "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "    data_url = hellaswags[split]\n",
    "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "    if not os.path.exists(data_filename):\n",
    "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "        download_file(data_url, data_filename)\n",
    "\n",
    "def iterate_examples(split):\n",
    "    download(split)\n",
    "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "\n",
    "def render_example(example):\n",
    "    \"\"\"Same as your GPT-2 code, returns (data, tokens, mask, label).\"\"\"\n",
    "    ctx = example[\"ctx\"]\n",
    "    label = example[\"label\"]\n",
    "    endings = example[\"endings\"]\n",
    "\n",
    "    data = {\n",
    "        \"label\": label,\n",
    "        \"ctx_tokens\": None,\n",
    "        \"ending_tokens\": [],\n",
    "    }\n",
    "    # Tokenize context\n",
    "    ctx_tokens = enc.encode(ctx)\n",
    "    data[\"ctx_tokens\"] = ctx_tokens\n",
    "\n",
    "    # Tokenize each of the 4 endings\n",
    "    tok_rows = []\n",
    "    mask_rows = []\n",
    "    for end in endings:\n",
    "        end_tokens = enc.encode(\" \" + end)  # note: leading space\n",
    "        tok_rows.append(ctx_tokens + end_tokens)\n",
    "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "        data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "    # Collate into a 4 x max_len shape\n",
    "    max_len = max(len(row) for row in tok_rows)\n",
    "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "    return data, tokens, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_iGPT(model_module, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Loads your trained iGPT model from a Lightning checkpoint, then\n",
    "    evaluates on the HellaSwag val set. We measure two scores:\n",
    "\n",
    "    1) 'acc' using sum of cross-entropy loss over each candidate (pred = argmin sum).\n",
    "    2) 'acc_norm' using average cross-entropy over each candidate region (pred = argmin mean).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # For convenience, we'll directly call model_module.network() \n",
    "    # inside our loop below. If your model requires anything special \n",
    "    # for generation or inference, adapt as needed.\n",
    "\n",
    "    # 2) Setup counters\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # 3) Iterate over HellaSwag val examples\n",
    "    for example in iterate_examples(\"val\"):\n",
    "        data, tokens, mask, label = render_example(example)\n",
    "        tokens = tokens.to(device)  # shape: (4, max_len)\n",
    "        mask = mask.to(device)      # shape: (4, max_len)\n",
    "\n",
    "        # iGPT expects two inputs: x, ix\n",
    "        # We'll feed 'tokens' as x, and supply a dummy 'ix' \n",
    "        # because we don't have any \"idea tokens\" at test time.\n",
    "        B, T = tokens.size()  # B = 4\n",
    "        ix_dummy = torch.zeros((B, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass to get logits: shape (B, T, vocab_size)\n",
    "        logits = model_module((tokens, ix_dummy))\n",
    "\n",
    "        # 4) Autoregressive loss\n",
    "        # same procedure as the GPT-2 script\n",
    "        shift_logits = logits[..., :-1, :].contiguous()   # (4, T-1, vocab_size)\n",
    "        shift_tokens = tokens[..., 1:].contiguous()       # (4, T-1)\n",
    "        flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "        flat_shift_tokens = shift_tokens.view(-1)\n",
    "        shift_losses = F.cross_entropy(\n",
    "            flat_shift_logits, \n",
    "            flat_shift_tokens,\n",
    "            reduction='none'\n",
    "        )\n",
    "        shift_losses = shift_losses.view(B, -1)  # shape (4, T-1)\n",
    "\n",
    "        # apply the mask\n",
    "        shift_mask = mask[..., 1:].contiguous()  # also shape (4, T-1)\n",
    "        masked_shift_losses = shift_losses * shift_mask\n",
    "\n",
    "        # sum/average over the candidate region\n",
    "        sum_loss = masked_shift_losses.sum(dim=1)          # shape (4,)\n",
    "        avg_loss = sum_loss / shift_mask.sum(dim=1)        # shape (4,)\n",
    "        \n",
    "        # the best candidate is the one with the lowest sum_loss\n",
    "        pred = sum_loss.argmin().item()\n",
    "        pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "        # 5) Accumulate stats\n",
    "        num_total += 1\n",
    "        num_correct += int(pred == label)\n",
    "        num_correct_norm += int(pred_norm == label)\n",
    "\n",
    "        # (Optional) debug prints\n",
    "        # if num_total <= 10:\n",
    "        #     print(\"---\")\n",
    "        #     print(f\"Context:\\n {example['ctx']}\")\n",
    "        #     print(\"Endings:\")\n",
    "        #     for i, end in enumerate(example[\"endings\"]):\n",
    "        #         print(f\"{i} (loss: {avg_loss[i].item():.4f}) => {end}\")\n",
    "        #     print(f\"Predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "        # (Optional) show running accuracy\n",
    "        if num_total % 100 == 0:\n",
    "            print(f\"Processed {num_total} examples. \" \n",
    "                  f\"acc: {num_correct/num_total:.4f}, \"\n",
    "                  f\"acc_norm: {num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "    # 6) Final results\n",
    "    print(f\"Done! Evaluated {num_total} examples.\")\n",
    "    acc = num_correct/num_total\n",
    "    acc_norm = num_correct_norm/num_total\n",
    "    print(f\"Final Acc: {acc:.4f}  Acc_norm: {acc_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples. acc: 0.2500, acc_norm: 0.1800\n",
      "Processed 200 examples. acc: 0.2750, acc_norm: 0.2450\n",
      "Processed 300 examples. acc: 0.2867, acc_norm: 0.2667\n",
      "Processed 400 examples. acc: 0.2650, acc_norm: 0.2725\n",
      "Processed 500 examples. acc: 0.2640, acc_norm: 0.2680\n",
      "Processed 600 examples. acc: 0.2950, acc_norm: 0.2867\n",
      "Processed 700 examples. acc: 0.2871, acc_norm: 0.2800\n",
      "Processed 800 examples. acc: 0.2750, acc_norm: 0.2900\n",
      "Processed 900 examples. acc: 0.2756, acc_norm: 0.2967\n",
      "Processed 1000 examples. acc: 0.2720, acc_norm: 0.2910\n",
      "Processed 1100 examples. acc: 0.2682, acc_norm: 0.2882\n",
      "Processed 1200 examples. acc: 0.2717, acc_norm: 0.2850\n",
      "Processed 1300 examples. acc: 0.2746, acc_norm: 0.2815\n",
      "Processed 1400 examples. acc: 0.2729, acc_norm: 0.2800\n",
      "Processed 1500 examples. acc: 0.2653, acc_norm: 0.2733\n",
      "Processed 1600 examples. acc: 0.2637, acc_norm: 0.2669\n",
      "Processed 1700 examples. acc: 0.2718, acc_norm: 0.2641\n",
      "Processed 1800 examples. acc: 0.2756, acc_norm: 0.2650\n",
      "Processed 1900 examples. acc: 0.2711, acc_norm: 0.2642\n",
      "Processed 2000 examples. acc: 0.2730, acc_norm: 0.2640\n",
      "Processed 2100 examples. acc: 0.2714, acc_norm: 0.2690\n",
      "Processed 2200 examples. acc: 0.2736, acc_norm: 0.2677\n",
      "Processed 2300 examples. acc: 0.2748, acc_norm: 0.2696\n",
      "Processed 2400 examples. acc: 0.2812, acc_norm: 0.2700\n",
      "Processed 2500 examples. acc: 0.2816, acc_norm: 0.2700\n",
      "Processed 2600 examples. acc: 0.2800, acc_norm: 0.2688\n",
      "Processed 2700 examples. acc: 0.2793, acc_norm: 0.2685\n",
      "Processed 2800 examples. acc: 0.2836, acc_norm: 0.2675\n",
      "Processed 2900 examples. acc: 0.2807, acc_norm: 0.2669\n",
      "Processed 3000 examples. acc: 0.2853, acc_norm: 0.2683\n",
      "Processed 3100 examples. acc: 0.2868, acc_norm: 0.2674\n",
      "Processed 3200 examples. acc: 0.2878, acc_norm: 0.2687\n",
      "Processed 3300 examples. acc: 0.2855, acc_norm: 0.2697\n",
      "Processed 3400 examples. acc: 0.2850, acc_norm: 0.2691\n",
      "Processed 3500 examples. acc: 0.2823, acc_norm: 0.2689\n",
      "Processed 3600 examples. acc: 0.2825, acc_norm: 0.2700\n",
      "Processed 3700 examples. acc: 0.2814, acc_norm: 0.2692\n",
      "Processed 3800 examples. acc: 0.2818, acc_norm: 0.2689\n",
      "Processed 3900 examples. acc: 0.2803, acc_norm: 0.2690\n",
      "Processed 4000 examples. acc: 0.2810, acc_norm: 0.2710\n",
      "Processed 4100 examples. acc: 0.2800, acc_norm: 0.2715\n",
      "Processed 4200 examples. acc: 0.2810, acc_norm: 0.2698\n",
      "Processed 4300 examples. acc: 0.2807, acc_norm: 0.2707\n",
      "Processed 4400 examples. acc: 0.2802, acc_norm: 0.2695\n",
      "Processed 4500 examples. acc: 0.2807, acc_norm: 0.2704\n",
      "Processed 4600 examples. acc: 0.2798, acc_norm: 0.2709\n",
      "Processed 4700 examples. acc: 0.2783, acc_norm: 0.2698\n",
      "Processed 4800 examples. acc: 0.2775, acc_norm: 0.2710\n",
      "Processed 4900 examples. acc: 0.2778, acc_norm: 0.2704\n",
      "Processed 5000 examples. acc: 0.2784, acc_norm: 0.2690\n",
      "Processed 5100 examples. acc: 0.2782, acc_norm: 0.2694\n",
      "Processed 5200 examples. acc: 0.2785, acc_norm: 0.2690\n",
      "Processed 5300 examples. acc: 0.2781, acc_norm: 0.2696\n",
      "Processed 5400 examples. acc: 0.2780, acc_norm: 0.2691\n",
      "Processed 5500 examples. acc: 0.2765, acc_norm: 0.2691\n",
      "Processed 5600 examples. acc: 0.2755, acc_norm: 0.2686\n",
      "Processed 5700 examples. acc: 0.2744, acc_norm: 0.2686\n",
      "Processed 5800 examples. acc: 0.2740, acc_norm: 0.2693\n",
      "Processed 5900 examples. acc: 0.2734, acc_norm: 0.2683\n",
      "Processed 6000 examples. acc: 0.2738, acc_norm: 0.2680\n",
      "Processed 6100 examples. acc: 0.2734, acc_norm: 0.2677\n",
      "Processed 6200 examples. acc: 0.2734, acc_norm: 0.2669\n",
      "Processed 6300 examples. acc: 0.2724, acc_norm: 0.2678\n",
      "Processed 6400 examples. acc: 0.2716, acc_norm: 0.2670\n",
      "Processed 6500 examples. acc: 0.2717, acc_norm: 0.2675\n",
      "Processed 6600 examples. acc: 0.2708, acc_norm: 0.2667\n",
      "Processed 6700 examples. acc: 0.2699, acc_norm: 0.2675\n",
      "Processed 6800 examples. acc: 0.2687, acc_norm: 0.2669\n",
      "Processed 6900 examples. acc: 0.2691, acc_norm: 0.2675\n",
      "Processed 7000 examples. acc: 0.2691, acc_norm: 0.2681\n",
      "Processed 7100 examples. acc: 0.2690, acc_norm: 0.2682\n",
      "Processed 7200 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7300 examples. acc: 0.2692, acc_norm: 0.2690\n",
      "Processed 7400 examples. acc: 0.2685, acc_norm: 0.2688\n",
      "Processed 7500 examples. acc: 0.2684, acc_norm: 0.2687\n",
      "Processed 7600 examples. acc: 0.2680, acc_norm: 0.2675\n",
      "Processed 7700 examples. acc: 0.2666, acc_norm: 0.2670\n",
      "Processed 7800 examples. acc: 0.2662, acc_norm: 0.2668\n",
      "Processed 7900 examples. acc: 0.2659, acc_norm: 0.2665\n",
      "Processed 8000 examples. acc: 0.2661, acc_norm: 0.2661\n",
      "Processed 8100 examples. acc: 0.2653, acc_norm: 0.2654\n",
      "Processed 8200 examples. acc: 0.2648, acc_norm: 0.2659\n",
      "Processed 8300 examples. acc: 0.2640, acc_norm: 0.2648\n",
      "Processed 8400 examples. acc: 0.2639, acc_norm: 0.2644\n",
      "Processed 8500 examples. acc: 0.2631, acc_norm: 0.2642\n",
      "Processed 8600 examples. acc: 0.2623, acc_norm: 0.2637\n",
      "Processed 8700 examples. acc: 0.2617, acc_norm: 0.2639\n",
      "Processed 8800 examples. acc: 0.2617, acc_norm: 0.2642\n",
      "Processed 8900 examples. acc: 0.2612, acc_norm: 0.2639\n",
      "Processed 9000 examples. acc: 0.2611, acc_norm: 0.2634\n",
      "Processed 9100 examples. acc: 0.2619, acc_norm: 0.2634\n",
      "Processed 9200 examples. acc: 0.2617, acc_norm: 0.2635\n",
      "Processed 9300 examples. acc: 0.2614, acc_norm: 0.2635\n",
      "Processed 9400 examples. acc: 0.2610, acc_norm: 0.2638\n",
      "Processed 9500 examples. acc: 0.2612, acc_norm: 0.2636\n",
      "Processed 9600 examples. acc: 0.2607, acc_norm: 0.2635\n",
      "Processed 9700 examples. acc: 0.2606, acc_norm: 0.2638\n",
      "Processed 9800 examples. acc: 0.2599, acc_norm: 0.2636\n",
      "Processed 9900 examples. acc: 0.2600, acc_norm: 0.2639\n",
      "Processed 10000 examples. acc: 0.2593, acc_norm: 0.2636\n",
      "Done! Evaluated 10042 examples.\n",
      "Final Acc: 0.2594  Acc_norm: 0.2634\n"
     ]
    }
   ],
   "source": [
    "evaluate_iGPT(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dcbc56aba04b8f80599bbe5f4853df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch=3-step=83166.ckpt:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a4ad8b17214360a9158b8aa516d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch=3-step=83266.ckpt:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e75e7b44fc5486ba263b551352d42b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1737308482.trustworthy-mbzuai.3253248.0:   0%|          | 0.00/26.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45b7a1ec6a49d28258c1fcbc1950b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"phusroyal/my-lightning-checkpoints\"\n",
    "folder_path = \"experiments/lightning_logs/version_010\"  # wherever your files are\n",
    "\n",
    "# If you haven't created the repo yet:\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Upload all files in `folder_path` to the Hugging Face Hub\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=folder_path,\n",
    "    commit_message=\"Upload igptv010\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phusr/miniconda3/envs/gpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotMyModel(\n",
       "  (network): iGPT(\n",
       "    (wte_i): Embedding(50304, 512)\n",
       "    (wpe_i): Embedding(256, 512)\n",
       "    (blocks_i): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (idea_head): Linear(in_features=512, out_features=768, bias=False)\n",
       "    (wte_g): Embedding(50304, 768)\n",
       "    (wpe_g): Embedding(256, 768)\n",
       "    (blocks_g): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f_g): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from model.iGPT import NotMyModel  # your custom LightningModule\n",
    "\n",
    "repo_id = \"phusroyal/my-lightning-checkpoints\"\n",
    "# Adjust the filename if your checkpoint file has a different name\n",
    "filename = \"checkpoints/epoch=4-step=13604.ckpt\"\n",
    "\n",
    "# This downloads the file and returns the local path\n",
    "ckpt_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Now load it in your LightningModule\n",
    "model_pl = NotMyModel.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "# Put model in eval mode, move to GPU/CPU, etc.\n",
    "model_pl.eval()\n",
    "model_pl.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
