{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This is the first sentence.',\n",
       "  'This is the second sentence.',\n",
       "  'This is the third sentence.',\n",
       "  'This is the fourth sentence.',\n",
       "  'This is the fifth sentence.'],\n",
       " ['This is the sixth sentence.',\n",
       "  'This is the seventh sentence.',\n",
       "  'This is the eighth sentence.',\n",
       "  'This is the ninth sentence.',\n",
       "  'This is the tenth sentence.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 2\n",
    "I = 5\n",
    "# Example sentences list\n",
    "sentences_list = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"This is the second sentence.\",\n",
    "    \"This is the third sentence.\",\n",
    "    \"This is the fourth sentence.\",\n",
    "    \"This is the fifth sentence.\", \n",
    "    \"This is the sixth sentence.\",\n",
    "    \"This is the seventh sentence.\",\n",
    "    \"This is the eighth sentence.\",\n",
    "    \"This is the ninth sentence.\",\n",
    "    \"This is the tenth sentence.\"\n",
    "]\n",
    "\n",
    "# Option 1: Use Python's list slicing\n",
    "sentences_reshaped = [sentences_list[i * I:(i + 1) * I] for i in range(B)]\n",
    "\n",
    "sentences_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "st_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "cache_folder = '/home/anwoy/phuhoang/ped/saved_models/sentence-transformers/all-mpnet-base-v2'\n",
    "local_file_only = True\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "sbert = SentenceTransformer(st_model_name,\n",
    "                                         device=device,\n",
    "                                        cache_folder=cache_folder,\n",
    "                                        local_files_only=local_file_only,\n",
    "                                        model_kwargs={\n",
    "                                            'torch_dtype': torch_dtype\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "tensor([[ 5.1659e-02, -5.9793e-02,  2.1608e-03,  ..., -1.0040e-02,\n",
      "         -4.2099e-02, -2.0275e-02],\n",
      "        [ 6.1750e-02, -3.6361e-02,  5.6251e-04,  ..., -1.3233e-02,\n",
      "         -4.6793e-02, -2.9813e-02],\n",
      "        [ 9.5577e-02, -6.4551e-02, -4.1595e-03,  ...,  1.9570e-04,\n",
      "         -6.7684e-02, -2.2272e-02],\n",
      "        ...,\n",
      "        [ 3.1220e-02,  3.3037e-02, -3.3743e-03,  ...,  1.9906e-02,\n",
      "         -9.1838e-02, -2.3821e-02],\n",
      "        [ 2.1601e-02,  1.4657e-02,  8.7469e-05,  ...,  1.4291e-02,\n",
      "         -6.8486e-02, -1.6401e-02],\n",
      "        [ 2.1405e-02,  5.6179e-03, -6.9115e-03,  ...,  3.6593e-02,\n",
      "         -8.4533e-02, -1.7980e-02]], device='cuda:0')\n",
      "torch.Size([2, 5, 768])\n",
      "tensor([[[ 5.1659e-02, -5.9793e-02,  2.1608e-03,  ..., -1.0040e-02,\n",
      "          -4.2099e-02, -2.0275e-02],\n",
      "         [ 6.1750e-02, -3.6361e-02,  5.6251e-04,  ..., -1.3233e-02,\n",
      "          -4.6793e-02, -2.9813e-02],\n",
      "         [ 9.5577e-02, -6.4551e-02, -4.1595e-03,  ...,  1.9570e-04,\n",
      "          -6.7684e-02, -2.2272e-02],\n",
      "         [ 6.1133e-02,  4.5061e-03, -1.7514e-02,  ...,  1.1207e-02,\n",
      "          -7.5255e-02, -3.1393e-02],\n",
      "         [ 7.5497e-02, -1.8473e-02, -2.2365e-02,  ...,  1.1290e-02,\n",
      "          -6.8621e-02, -1.9517e-02]],\n",
      "\n",
      "        [[ 9.6406e-02, -8.9544e-03, -9.9801e-03,  ...,  1.9112e-02,\n",
      "          -7.1171e-02, -1.2197e-02],\n",
      "         [ 5.6731e-02, -3.4250e-02, -3.5807e-03,  ...,  1.2702e-02,\n",
      "          -4.6963e-02, -1.6760e-02],\n",
      "         [ 3.1220e-02,  3.3037e-02, -3.3743e-03,  ...,  1.9906e-02,\n",
      "          -9.1838e-02, -2.3821e-02],\n",
      "         [ 2.1601e-02,  1.4657e-02,  8.7469e-05,  ...,  1.4291e-02,\n",
      "          -6.8486e-02, -1.6401e-02],\n",
      "         [ 2.1405e-02,  5.6179e-03, -6.9115e-03,  ...,  3.6593e-02,\n",
      "          -8.4533e-02, -1.7980e-02]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = sbert.encode(sentences_list, convert_to_tensor=True)\n",
    "print(sentence.shape)  # Should print torch.Size([B, I, 768]) if B=2 and I=5\n",
    "print(sentence)\n",
    "reshaped_sentence = sentence.view(B, I, -1)  # Reshape to (B, I, 768)\n",
    "print(reshaped_sentence.shape)  # Should print torch.Size([2, 5, 768])\n",
    "print(reshaped_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import lightning as L\n",
    "from dataclasses import dataclass\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from lightning.pytorch.utilities import grad_norm\n",
    "\n",
    "# from model.transformer_blocks import Block, CausalSelfAttention, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)                   # (B, T, 3C)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(n_embd * mlp_ratio)\n",
    "        self.c_fc = nn.Linear(n_embd, hidden_dim)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(hidden_dim, n_embd)\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # removed the 'idea_vector' argument / logic\n",
    "    def __init__(self, n_embd, n_head, mlp_ratio=4.0, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head)\n",
    "        self.ln_2 = norm_layer(n_embd)\n",
    "        self.mlp = MLP(n_embd, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedIdearEncoder(nn.Module):\n",
    "    def __init__(self, st_model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        # Load the Sentence Transformer model\n",
    "        self.sbert = SentenceTransformer(st_model_name)\n",
    "        # The output dimension is typically 768\n",
    "        self.sbert_dim = 768\n",
    "\n",
    "    def forward(self, sentence_list):\n",
    "        # Get the idea embeddings from all-mpnet-base-v2\n",
    "        with torch.no_grad():\n",
    "            idea_vecs = self.sbert.encode(sentence_list, convert_to_tensor=True)  # shape (B, 768)\n",
    "        return idea_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaPredictor(nn.Module):\n",
    "    def __init__(self, \n",
    "                block_size=32,\n",
    "                input_dim=768,\n",
    "                predictor_embed_dim=384,\n",
    "                predictor_depth=12,\n",
    "                predictor_num_heads=12,\n",
    "                mlp_ratio=4.0,\n",
    "                norm_layer=nn.LayerNorm,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize the IdeaPredictor model.\n",
    "        Args:\n",
    "            block_size (int): The maximum number of sequences for the predictor.\n",
    "            input_dim (int): The dimension of the input idea embeddings.\n",
    "            predictor_embed_dim (int): The embedding dimension for the predictor.\n",
    "            predictor_depth (int): The number of transformer blocks in the predictor.\n",
    "            predictor_num_heads (int): The number of attention heads in each transformer block.\n",
    "            mlp_ratio (float): The ratio of the hidden dimension to the embedding dimension in the MLP.\n",
    "            norm_layer (nn.Module): The normalization layer to use.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.input_dim = input_dim\n",
    "        self.predictor_embed_dim = predictor_embed_dim\n",
    "        self.predictor_depth = predictor_depth\n",
    "        self.predictor_num_heads = predictor_num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        # Project from idea encoder dimension to predictor dimension\n",
    "        if predictor_embed_dim != input_dim:\n",
    "            self.idea_proj = nn.Linear(input_dim, predictor_embed_dim, bias=False)            \n",
    "            self.predictor_proj = nn.Linear(predictor_embed_dim, input_dim, bias=False)\n",
    "            self.predictor_proj.SCALE_INIT = 1\n",
    "            self.idea_proj.SCALE_INIT = 1\n",
    "        else:\n",
    "            self.idea_proj = nn.Identity()\n",
    "            self.predictor_proj = nn.Identity()\n",
    "\n",
    "        # Use a larger positional embedding to handle variable number of ideas\n",
    "        max_ideas = 512  # Allow for up to 512 ideas\n",
    "        self.predictor_wpe = nn.Embedding(max_ideas, predictor_embed_dim)\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                n_embd=predictor_embed_dim,\n",
    "                n_head=predictor_num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                # qkv_bias=qkv_bias,\n",
    "                # qk_scale=qk_scale,\n",
    "                # drop_rate=drop_rate,\n",
    "                # attn_drop_rate=attn_drop_rate,\n",
    "                # drop_path_rate=drop_path_rate,\n",
    "            )\n",
    "            for _ in range(predictor_depth)\n",
    "        ])\n",
    "\n",
    "        self.predictor_ln = nn.LayerNorm(predictor_embed_dim)\n",
    "\n",
    "        # might need weight tying\n",
    "        # what and why is weight tying?\n",
    "        # what: Weight tying is a technique where the same weight matrix is used for both the input and output projections in a model, reducing the number of parameters and potentially improving generalization.\n",
    "        # why: This can help prevent overfitting, especially in models with large vocabularies or embedding spaces, by forcing the model to learn a more compact representation.\n",
    "        # as idea prediction is a regression task, we do not use weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.predictor_depth) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idea_vecs, target=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the IdeaPredictor model.\n",
    "        Args:\n",
    "            idea_vecs (torch.Tensor): Input idea embeddings of shape (B, I, input_dim), where B is the batch size and I is the number of ideas.\n",
    "            target (torch.Tensor, optional): Target values for training. Not used in this implementation.\n",
    "        \"\"\"\n",
    "        B, I = idea_vecs.size(0), idea_vecs.size(1)\n",
    "\n",
    "        # Project the idea embeddings\n",
    "        idea_emb = self.idea_proj(idea_vecs)  # shape (B, I, predictor_embed_dim)\n",
    "\n",
    "        # Add positional embeddings - ensure we don't exceed embedding size\n",
    "        pos_ids = torch.arange(0, I, device=idea_vecs.device)  # length I\n",
    "        pos_emb = self.predictor_wpe(pos_ids)  # shape (I, predictor_embed_dim)\n",
    "        hidden = idea_emb + pos_emb.unsqueeze(0)  # (B, I, predictor_embed_dim)\n",
    "\n",
    "        # Forward through predictor blocks\n",
    "        for block in self.predictor_blocks:\n",
    "            hidden = block(hidden)\n",
    "        hidden = self.predictor_ln(hidden)\n",
    "\n",
    "        # Project back to the original dimension\n",
    "        hidden = self.predictor_proj(hidden)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                block_size=32,\n",
    "                vocab_size=50304,\n",
    "                input_dim=768,\n",
    "                decoder_embed_dim=384,\n",
    "                decoder_depth=6,\n",
    "                decoder_num_heads=12,\n",
    "                mlp_ratio=4.0,\n",
    "                # qkv_bias=False,\n",
    "                # qk_scale=None,\n",
    "                # drop_rate=0.0,\n",
    "                # attn_drop_rate=0.0,\n",
    "                # drop_path_rate=0.0,\n",
    "                # init_std=0.02,\n",
    "                norm_layer=nn.LayerNorm,\n",
    "                ):\n",
    "        \"\"\" Initialize the IdeaDecoder model.\n",
    "        Args:\n",
    "            block_size (int): The maximum number of tokens for the decoder.\n",
    "            vocab_size (int): The size of the vocabulary for the decoder.\n",
    "            input_dim (int): The dimension of the input idea embeddings.\n",
    "            decoder_embed_dim (int): The embedding dimension for the decoder.\n",
    "            decoder_depth (int): The number of transformer blocks in the decoder.\n",
    "            decoder_num_heads (int): The number of attention heads in each transformer block.\n",
    "            mlp_ratio (float): The ratio of the hidden dimension to the embedding dimension in the MLP.\n",
    "            norm_layer (nn.Module): The normalization layer to use.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.input_dim = input_dim\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_depth = decoder_depth\n",
    "        self.decoder_num_heads = decoder_num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm_layer = norm_layer\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Project from idea encoder dimension to predictor dimension\n",
    "        if decoder_embed_dim != input_dim:\n",
    "            self.idea_proj = nn.Linear(input_dim, decoder_embed_dim, bias=False)            \n",
    "            self.decoder_proj = nn.Linear(decoder_embed_dim, input_dim, bias=False)\n",
    "            self.decoder_proj.SCALE_INIT = 1\n",
    "            self.idea_proj.SCALE_INIT = 1\n",
    "        else:\n",
    "            self.idea_proj = nn.Identity()\n",
    "            self.decoder_proj = nn.Identity()\n",
    "\n",
    "        self.decoder_wte = nn.Embedding(vocab_size, decoder_embed_dim)\n",
    "        self.decoder_wpe = nn.Embedding(block_size+1, decoder_embed_dim)\n",
    "        self.block = nn.ModuleList([\n",
    "            Block(\n",
    "                n_embd=decoder_embed_dim,\n",
    "                n_head=decoder_num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                # qkv_bias=qkv_bias,\n",
    "                # qk_scale=qk_scale,\n",
    "                # drop_rate=drop_rate,\n",
    "                # attn_drop_rate=attn_drop_rate,\n",
    "                # drop_path_rate=drop_path_rate,\n",
    "            )\n",
    "            for _ in range(decoder_depth)\n",
    "        ])\n",
    "\n",
    "        self.decoder_ln = nn.LayerNorm(decoder_embed_dim)\n",
    "        self.decoder_lm_head = nn.Linear(decoder_embed_dim, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.decoder_wte.weight = self.decoder_lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std *= (2 * self.decoder_depth) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, idea_vec, target=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the IdeaDecoder model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token indices of shape (B*I, T), where B is the batch size and I is the number of ideas.\n",
    "            idea_vec (torch.Tensor): Input idea embeddings of shape (B*I, input_dim).\n",
    "            target (torch.Tensor, optional): Target values for training. Not used in this implementation.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits for next-word prediction of shape (B*I, T, vocab_size).   \n",
    "        \"\"\"\n",
    "        B, T = x.size()   # now B and T are ints\n",
    "\n",
    "        # Project the idea embeddings\n",
    "        idea_vec = self.idea_proj(idea_vec)\n",
    "\n",
    "        # forward the token and positional embeddings\n",
    "        token_emb = self.decoder_wte(x)  # (B*I, T, decoder_embed_dim)\n",
    "        pos_ids = torch.arange(0, T, device=x.device)\n",
    "        pos_emb = self.decoder_wpe(pos_ids)       \n",
    "        idea_token = idea_vec.unsqueeze(1)                 # (B*I, 1, D)\n",
    "        token_and_pos = token_emb + pos_emb               # (B*I, T, D)\n",
    "        hidden = torch.cat([idea_token, token_and_pos], 1) # (B*I, T+1, D)\n",
    "\n",
    "\n",
    "        # Forward through decoder blocks\n",
    "        for block in self.block:\n",
    "            hidden = block(hidden)\n",
    "\n",
    "        # Normalize the hidden states\n",
    "        hidden = self.decoder_ln(hidden)\n",
    "\n",
    "        # skip the idea token when producing logits for next-word prediction\n",
    "        logits = self.decoder_lm_head(hidden[:, 1:, :])  # shape (B*I, T, vocab_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPTConfig:\n",
    "    id_pred_block_size: int = 32\n",
    "    id_pred_n_layer: int = 6\n",
    "    id_pred_n_head: int = 12\n",
    "    id_pred_n_embd: int = 384\n",
    "    id_dec_block_size: int = 32\n",
    "    id_dec_n_layer: int = 6\n",
    "    id_dec_n_head: int = 12\n",
    "    id_dec_n_embd: int = 384\n",
    "    mlp_ratio: float = 4.0\n",
    "    vocab_size: int = 50304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT model that prepends a single 'idea token' derived from\n",
    "    all-mpnet-base-v2 embeddings at the start of the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: iGPTConfig, st_model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # -- idea encoder\n",
    "        # Load the Sentence Transformer model\n",
    "        self.sbert = SentenceTransformer(st_model_name)\n",
    "        self.sbert.eval()  # Set to evaluation mode\n",
    "        self.sbert_dim = self.sbert.get_sentence_embedding_dimension()  # typically 768\n",
    "        \n",
    "        # -- idea predictor\n",
    "        self.idea_predictor = IdeaPredictor(\n",
    "            block_size=config.id_pred_block_size,\n",
    "            input_dim=self.sbert_dim,\n",
    "            predictor_embed_dim=config.id_pred_n_embd,\n",
    "            predictor_depth=config.id_pred_n_layer,\n",
    "            predictor_num_heads=config.id_pred_n_head,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            norm_layer=nn.LayerNorm\n",
    "        )\n",
    "\n",
    "        # -- idea decoder\n",
    "        self.idea_decoder = IdeaDecoder(\n",
    "            block_size=config.id_dec_block_size,\n",
    "            vocab_size=config.vocab_size,\n",
    "            input_dim=self.sbert_dim,\n",
    "            decoder_embed_dim=config.id_dec_n_embd,\n",
    "            decoder_depth=config.id_dec_n_layer,\n",
    "            decoder_num_heads=config.id_dec_n_head,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            norm_layer=nn.LayerNorm\n",
    "        )\n",
    "\n",
    "    def forward(self, xt, xs):\n",
    "        \"\"\"\n",
    "        Forward pass for the iGPT model.\n",
    "        Args:\n",
    "            xt (torch.Tensor): Input token indices of shape (B*I, T), where B is the batch size and I is the number of ideas.\n",
    "            xs (torch.Tensor): Input idea embeddings of shape (B, I, sbert_dim) - pre-computed embeddings instead of sentences.\n",
    "        Returns:\n",
    "            yt (torch.Tensor): Logits for next-word prediction of shape (B*I, T, vocab_size).\n",
    "            ys (torch.Tensor): Idea embeddings of shape (B*I, sbert_dim).\n",
    "            idea_vecs (torch.Tensor): Original idea embeddings of shape (B, I, sbert_dim).\n",
    "        \"\"\"\n",
    "        batch_size, num_ideas, _ = xs.shape  # Get actual batch size and number of ideas\n",
    "        seq_length = xt.shape[1]\n",
    "        \n",
    "        print(f\"xt shape: {xt.shape}\")\n",
    "        print(f\"xs shape: {xs.shape}\")\n",
    "        print(f\"B: {batch_size}, I: {num_ideas}, T: {seq_length}\")\n",
    "\n",
    "        # xs is already the idea embeddings tensor, no need for sentence encoding\n",
    "        idea_vecs = xs  # shape (B, I, sbert_dim)\n",
    "        \n",
    "        # Pass to idea predictor\n",
    "        ys = self.idea_predictor(idea_vecs)  # shape (B, I, sbert_dim)\n",
    "        \n",
    "        # Reshape ys to match xt batch dimension: (B, I, sbert_dim) -> (B*I, sbert_dim)\n",
    "        ys_flat = ys.view(batch_size * num_ideas, -1)\n",
    "\n",
    "        # Pass to idea decoder\n",
    "        yt = self.idea_decoder(xt, ys_flat)\n",
    "\n",
    "        return yt, ys_flat, idea_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iGPTConfig:\n",
    "    id_pred_block_size = 2\n",
    "    id_pred_n_layer = 2\n",
    "    id_pred_n_head = 4\n",
    "    id_pred_n_embd = 128\n",
    "\n",
    "    id_dec_block_size = 2\n",
    "    id_dec_n_layer = 2\n",
    "    id_dec_n_head = 4\n",
    "    id_dec_n_embd = 128\n",
    "    \n",
    "    mlp_ratio = 4\n",
    "    vocab_size = 1000  # Reduced from 50304 for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_igpt():\n",
    "    # Small test configuration\n",
    "    test_config = iGPTConfig()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = iGPT(test_config)\n",
    "    \n",
    "    # Create sample inputs\n",
    "    batch_size = 2\n",
    "    num_ideas = 5\n",
    "    seq_length = test_config.id_dec_block_size  # FIX: The sequence length must not exceed the configured block size\n",
    "    sbert_dim = 768\n",
    "    \n",
    "    sample_tokens = torch.randint(0, test_config.vocab_size, (batch_size * num_ideas, seq_length))\n",
    "    sample_idea_embeddings = torch.randn(batch_size, num_ideas, sbert_dim)  # Random embeddings instead of sentences\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits, embeddings, _ = model(sample_tokens, sample_idea_embeddings)\n",
    "    \n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "    print(f\"Output embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xt shape: torch.Size([10, 2])\n",
      "xs shape: torch.Size([2, 5, 768])\n",
      "B: 2, I: 5, T: 2\n",
      "Output logits shape: torch.Size([10, 2, 1000])\n",
      "Output embeddings shape: torch.Size([10, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iGPT(\n",
       "  (sbert): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "    (2): Normalize()\n",
       "  )\n",
       "  (idea_predictor): IdeaPredictor(\n",
       "    (idea_proj): Linear(in_features=768, out_features=128, bias=False)\n",
       "    (predictor_proj): Linear(in_features=128, out_features=768, bias=False)\n",
       "    (predictor_wpe): Embedding(512, 128)\n",
       "    (predictor_blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predictor_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (idea_decoder): IdeaDecoder(\n",
       "    (idea_proj): Linear(in_features=768, out_features=128, bias=False)\n",
       "    (decoder_proj): Linear(in_features=128, out_features=768, bias=False)\n",
       "    (decoder_wte): Embedding(1000, 128)\n",
       "    (decoder_wpe): Embedding(3, 128)\n",
       "    (block): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder_lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_igpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
